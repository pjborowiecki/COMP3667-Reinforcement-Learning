{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>The Ministiry Of Silly Walks</b></center>\n",
    "# <center>Learning To Walk Funny But Efficiently</center>\n",
    "### <center>COMP3667 Reinforcement Learning Assignment 2022/2023</cetner>\n",
    "#### <center>Piotr Borowiecki (svmm25)</center>\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2020, Randomized Ensembled Double Q-Learning: Learning Fast Without a Model (Chen, Wang, Zhou, Ross)](https://openreview.net/pdf?id=AY8zfZm0tDd)\n",
    "\n",
    "* [2018, Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (Haarnoja, Zhou, Abbeel, Levine)](https://arxiv.org/pdf/1801.01290.pdf)\n",
    "\n",
    "* [2016, Deep Reinforcement Learning with Double Q-Learning (van Hasselt, Guez, Silver)](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "\n",
    "* [2015, Human-Level Control Through Deep Reinforcement Learning (Mnih et al.)](https://www.nature.com/articles/nature14236/?source=post_page---------------------------)\n",
    "\n",
    "* [2013, Playing Atari with Deep Reinforcement Learning (Mnih et al.)](https://arxiv.org/pdf/1312.5602.pdf)\n",
    "\n",
    "* [2010, Double Q-Learning (van Hasselt)](https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf)\n",
    "\n",
    "* [1989, Learning from Delayed Rewards (Watkins)](https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards)\n",
    "\n",
    "<br>\n",
    "\n",
    "* [SAC-PLUS GitHub repository by LucasAlegre](https://github.com/LucasAlegre/sac-plus)\n",
    "\n",
    "* [REDQ repository by watchernyu](https://github.com/watchernyu/REDQ)\n",
    "\n",
    "* [Reinforcement Learning section of labml.ai](https://nn.labml.ai/rl/index.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay==3.0 in /Users/svmm25/opt/anaconda3/envs/reinforcement-learning/lib/python3.9/site-packages (3.0)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# !apt update\n",
    "# !apt install xvfb -y\n",
    "# !pip install 'swig'\n",
    "# !pip install 'pyglet==1.5.27'\n",
    "# !pip install 'gym[box2d]==0.20.0'\n",
    "# !pip install 'pyvirtualdisplay==3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and recording settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 1\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 2\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 3\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 4\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 5\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 6\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 7\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 8\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 9\n",
      "process exited early. stderr:b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"\n",
      "start failed 10\n"
     ]
    },
    {
     "ename": "XStartError",
     "evalue": "No success after 10 retries. Last stderr: b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXStartError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/reinforcement-learning/lib/python3.9/site-packages/pyvirtualdisplay/abstractdisplay.py:155\u001b[0m, in \u001b[0;36mAbstractDisplay.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reinforcement-learning/lib/python3.9/site-packages/pyvirtualdisplay/abstractdisplay.py:286\u001b[0m, in \u001b[0;36mAbstractDisplay._start1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to start process: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XStartError(msg \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n",
      "\u001b[0;31mXStartError\u001b[0m: Failed to start process: <pyvirtualdisplay.xvfb.XvfbDisplay object at 0x105f71190>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mXStartError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m display \u001b[38;5;241m=\u001b[39m Display(visible\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m600\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plot_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# update the plot every N episodes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m video_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;66;03m# videos can take a very long time to render so only do it every N episodes\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reinforcement-learning/lib/python3.9/site-packages/pyvirtualdisplay/display.py:72\u001b[0m, in \u001b[0;36mDisplay.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisplay\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    start display\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: self\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/reinforcement-learning/lib/python3.9/site-packages/pyvirtualdisplay/abstractdisplay.py:162\u001b[0m, in \u001b[0;36mAbstractDisplay.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m             i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries:\n\u001b[0;32m--> 162\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m XStartError(\n\u001b[1;32m    163\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo success after \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m retries. Last stderr: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    165\u001b[0m                 )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manage_global_env:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_redirect_display(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mXStartError\u001b[0m: No success after 10 retries. Last stderr: b\"_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\\n_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\\n_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\\n(EE) \\nFatal server error:\\n(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \\n\""
     ]
    }
   ],
   "source": [
    "display = Display(visible=0, size=(600, 600))\n",
    "display.start()\n",
    "\n",
    "plot_interval = 2 # update the plot every N episodes\n",
    "video_every = 20 # videos can take a very long time to render so only do it every N episodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "# env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
    "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
    "\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "maximum_action = env.action_space.high[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The environment has {} observations and the agent can take {} actions'.format(observation_space, action_space))\n",
    "print('The device is: {}'.format(device))\n",
    "\n",
    "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            output_size,\n",
    "            hidden_dimensions,\n",
    "            hidden_activation=F.relu\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        input_size = input_size\n",
    "\n",
    "        for i, next_size in enumerate(hidden_dimensions):\n",
    "            fc_layer = nn.Linear(input_size, next_size)\n",
    "            input_size = next_size\n",
    "            self.hidden_layers.append(fc_layer)\n",
    "\n",
    "        self.last_fc_layer = nn.Linear(input_size, output_size)\n",
    "        self.apply(self.weights_init_)\n",
    "\n",
    "    def weights_init_(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = input\n",
    "        for i, fc_layer in enumerate(self.hidden_layers):\n",
    "            h = fc_layer(h)\n",
    "            h = self.hidden_activation(h)\n",
    "        output = self.last_fc_layer(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_size,\n",
    "#             output_size,\n",
    "#             hidden_dimensions,\n",
    "#             hidden_activation=F.relu,\n",
    "#             dropout_prob=0.2,\n",
    "#             use_batch_norm=True,\n",
    "#             use_skip_connection=True,\n",
    "#             dueling_architecture=True,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.output_size = output_size\n",
    "#         self.hidden_activation = hidden_activation\n",
    "#         self.dropout_prob = dropout_prob\n",
    "#         self.use_batch_norm = use_batch_norm\n",
    "#         self.use_skip_connection = use_skip_connection\n",
    "#         self.dueling_architecture = dueling_architecture\n",
    "        \n",
    "#         self.hidden_layers = nn.ModuleList()\n",
    "#         input_size = input_size\n",
    "        \n",
    "#         for i, next_size in enumerate(hidden_dimensions):\n",
    "#             fc_layer = nn.Linear(input_size, next_size)\n",
    "#             input_size = next_size\n",
    "            \n",
    "#             if self.use_batch_norm:\n",
    "#                 bn_layer = nn.BatchNorm1d(next_size)\n",
    "#                 self.hidden_layers.append(nn.Sequential(fc_layer, bn_layer))\n",
    "#             else:\n",
    "#                 self.hidden_layers.append(fc_layer)\n",
    "                \n",
    "#             if self.use_skip_connection and i < len(hidden_dimensions) - 1:\n",
    "#                 skip_layer = nn.Linear(input_size, next_size)\n",
    "#                 self.hidden_layers.append(skip_layer)\n",
    "                \n",
    "#         if self.dueling_architecture:\n",
    "#             self.advantage_fc_layer = nn.Linear(input_size, output_size)\n",
    "#             self.value_fc_layer = nn.Linear(input_size, 1)\n",
    "#         else:\n",
    "#             self.last_fc_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "#         self.apply(self.weights_init_)\n",
    "        \n",
    "#     def weights_init_(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "#             torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         h = input\n",
    "        \n",
    "#         for i, layer in enumerate(self.hidden_layers):\n",
    "#             if self.use_skip_connection and i % 2 == 1:\n",
    "#                 skip_layer_input = h\n",
    "#             h = layer(h)\n",
    "#             h = self.hidden_activation(h)\n",
    "#             if self.use_skip_connection and i % 2 == 0 and i > 0:\n",
    "#                 h = h + skip_layer_input\n",
    "        \n",
    "#         if self.dueling_architecture:\n",
    "#             advantage = self.advantage_fc_layer(h)\n",
    "#             value = self.value_fc_layer(h)\n",
    "#             q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "#         else:\n",
    "#             q_values = self.last_fc_layer(h)\n",
    "        \n",
    "#         return q_values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        maximum_action,\n",
    "        hidden_dimensions,\n",
    "        hidden_activation,\n",
    "        action_bound_epsilon,\n",
    "        log_sig_min,\n",
    "        log_sig_max       \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden_activation = hidden_activation\n",
    "\n",
    "        self.action_bound_epsilon = action_bound_epsilon\n",
    "        self.log_sig_min = log_sig_min\n",
    "        self.log_sig_max = log_sig_max\n",
    "\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        input_size = self.observation_space\n",
    "        for i, next_size in enumerate(hidden_dimensions):\n",
    "            fc_layer = nn.Linear(input_size, next_size)\n",
    "            input_size = next_size\n",
    "            self.hidden_layers.append(fc_layer)\n",
    "\n",
    "        self.last_fc_layer = nn.Linear(input_size, self.action_space)\n",
    "        self.last_fc_log_std = nn.Linear(input_size, self.action_space)\n",
    "\n",
    "        self.maximum_action = maximum_action\n",
    "\n",
    "        self.apply(self.weights_init_)\n",
    "\n",
    "    def weights_init_(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            observation,\n",
    "            deterministic=False,\n",
    "            with_log_prob=True,\n",
    "    ):\n",
    "        h = observation\n",
    "        for fc_layer in self.hidden_layers:\n",
    "            h = self.hidden_activation(fc_layer(h))\n",
    "        mean = self.last_fc_layer(h)\n",
    "\n",
    "        log_std = self.last_fc_log_std(h)\n",
    "        log_std = torch.clamp(log_std, self.log_sig_min, self.log_sig_max)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        normal = Normal(mean, std)\n",
    "\n",
    "        if deterministic:\n",
    "            pre_tanh_value = mean\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            pre_tanh_value = normal.rsample()\n",
    "            action = torch.tanh(pre_tanh_value)\n",
    "\n",
    "        if with_log_prob:\n",
    "            log_prob = normal.log_prob(pre_tanh_value)\n",
    "            log_prob -= torch.log(1 - action.pow(2) + self.action_bound_epsilon)\n",
    "            log_prob = log_prob.sum(1, keepdim=True)\n",
    "        else:\n",
    "            log_prob = None\n",
    "\n",
    "        return (\n",
    "            action * self.maximum_action, mean, log_std, log_prob, std, pre_tanh_value,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(\n",
    "        self, \n",
    "        observation_space, \n",
    "        action_space, \n",
    "        size\n",
    "    ):\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.observation_1_buffer = np.zeros([size, self.observation_space], dtype=np.float32)\n",
    "        self.observation_2_buffer = np.zeros([size, self.observation_space], dtype=np.float32)\n",
    "        self.actions_buffer = np.zeros([size, self.action_space], dtype=np.float32)\n",
    "        self.rewards_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.position = 0 \n",
    "        self.size = 0\n",
    "        self.max_size = size\n",
    "        \n",
    "    def store(\n",
    "        self, \n",
    "        observation, \n",
    "        action, \n",
    "        reward, \n",
    "        next_observation, \n",
    "        done\n",
    "    ):\n",
    "   \n",
    "        self.observation_1_buffer[self.position] = observation\n",
    "        self.observation_2_buffer[self.position] = next_observation\n",
    "        self.actions_buffer[self.position] = action\n",
    "        self.rewards_buffer[self.position] = reward\n",
    "        self.done_buffer[self.position] = done\n",
    "\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "    def sample_batch(\n",
    "        self, \n",
    "        batch_size, \n",
    "        idxs=None\n",
    "    ):\n",
    "        if idxs is None:\n",
    "            idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return dict(observation_1=self.observation_1_buffer[idxs],\n",
    "                    observation_2=self.observation_2_buffer[idxs],\n",
    "                    actions=self.actions_buffer[idxs],\n",
    "                    rewards=self.rewards_buffer[idxs],\n",
    "                    done=self.done_buffer[idxs],\n",
    "                    idxs=idxs\n",
    "                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        observation_space, \n",
    "        action_space, \n",
    "        maximum_action,\n",
    "        hidden_dimensions, \n",
    "        hidden_activation,\n",
    "        action_bound_epsilon,\n",
    "        log_sig_min,\n",
    "        log_sig_max,\n",
    "        N, \n",
    "        M, \n",
    "        G,\n",
    "        replay_size, \n",
    "        batch_size,\n",
    "        alpha, \n",
    "        gamma, \n",
    "        polyak,\n",
    "        random_action_steps, \n",
    "        policy_update_delay,\n",
    "        learning_rate, \n",
    "        device\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.maximum_action = maximum_action\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.action_bound_epsilon = action_bound_epsilon\n",
    "        self.log_sig_min = log_sig_min\n",
    "        self.log_sig_max = log_sig_max\n",
    "\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.G = G\n",
    "\n",
    "        self.replay_size = replay_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.polyak = polyak\n",
    "\n",
    "        self.random_action_steps = random_action_steps\n",
    "        self.delay_update_steps = self.random_action_steps\n",
    "        self.policy_update_delay = policy_update_delay\n",
    "        self.target_entropy = -self.maximum_action\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "\n",
    "        \n",
    "        # NETWORKS\n",
    "        self.policy_network = PolicyNetwork(\n",
    "            observation_space=self.observation_space, \n",
    "            action_space=self.action_space, \n",
    "            maximum_action=self.maximum_action,\n",
    "            hidden_dimensions=self.hidden_dimensions, \n",
    "            hidden_activation=self.hidden_activation,\n",
    "            action_bound_epsilon=self.action_bound_epsilon,\n",
    "            log_sig_min=self.log_sig_min,\n",
    "            log_sig_max=self.log_sig_max\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        self.q_networks_list, self.q_target_networks_list = [], []\n",
    "        for q_i in range(self.N):\n",
    "            new_q_net = QNetwork(\n",
    "                input_size=self.observation_space + self.action_space, \n",
    "                output_size=1, \n",
    "                hidden_dimensions=self.hidden_dimensions\n",
    "            ).to(device)\n",
    "            self.q_networks_list.append(new_q_net)\n",
    "\n",
    "            new_q_target_network = QNetwork(\n",
    "                input_size=self.observation_space + self.action_space, \n",
    "                output_size=1, \n",
    "                hidden_dimensions=self.hidden_dimensions\n",
    "            ).to(device)\n",
    "            new_q_target_network.load_state_dict(new_q_net.state_dict())\n",
    "            self.q_target_networks_list.append(new_q_target_network)\n",
    "\n",
    "        # OPTIMIZERS\n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=self.learning_rate)\n",
    "        self.q_optimizer_list = []\n",
    "        for q_i in range(self.N):\n",
    "            self.q_optimizer_list.append(optim.Adam(self.q_networks_list[q_i].parameters(), lr=self.learning_rate))\n",
    "\n",
    "\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=self.learning_rate)\n",
    "        self.alpha = self.log_alpha.cpu().exp().item()\n",
    "\n",
    "        # EXPERIENCE REPLAY\n",
    "        self.experience_replay = ExperienceReplay(\n",
    "            observation_space=self.observation_space, \n",
    "            action_space=self.action_space, \n",
    "            size=self.replay_size\n",
    "        )\n",
    "\n",
    "        # LOSS\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def __get_current_num_data(self):\n",
    "        return self.experience_replay.size\n",
    "\n",
    "\n",
    "    def get_exploration_action(\n",
    "        self, \n",
    "        observation, \n",
    "        env\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            if self.__get_current_num_data() > self.random_action_steps:\n",
    "                observation_tensor = torch.Tensor(observation).unsqueeze(0).to(self.device)\n",
    "                action_tensor = self.policy_network.forward(observation_tensor, deterministic=False, with_log_prob=False)[0]\n",
    "                action = action_tensor.cpu().numpy().reshape(-1)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def get_test_action(\n",
    "        self, \n",
    "        observation\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            observation_tensor = torch.Tensor(observation).unsqueeze(0).to(self.device)\n",
    "            action_tensor = self.policy_network.forward(observation_tensor, deterministic=True, with_log_prob=False)[0]\n",
    "            action = action_tensor.cpu().numpy().reshape(-1)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_action_and_logprob_for_bias_evaluation(\n",
    "        self, \n",
    "        observation\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            observation_tensor = torch.Tensor(observation).unsqueeze(0).to(self.device)\n",
    "            action_tensor, _, _, log_prob_a_tilda, _, _, = self.policy_network.forward(observation_tensor, deterministic=False, with_log_prob=True)\n",
    "            action = action_tensor.cpu().numpy().reshape(-1)\n",
    "        return action, log_prob_a_tilda\n",
    "\n",
    "\n",
    "    def get_ave_q_prediction_for_bias_evaluation(\n",
    "        self, \n",
    "        observation_tensor, \n",
    "        actions_tensor\n",
    "    ):\n",
    "        q_prediction_list = []\n",
    "        for q_i in range(self.N):\n",
    "            q_prediction = self.q_networks_list[q_i](torch.cat([observation_tensor, actions_tensor], 1))\n",
    "            q_prediction_list.append(q_prediction)\n",
    "        q_prediction_cat = torch.cat(q_prediction_list, dim=1)\n",
    "        average_q_prediction = torch.mean(q_prediction_cat, dim=1)\n",
    "        return average_q_prediction\n",
    "\n",
    "\n",
    "    def store_data(\n",
    "        self, \n",
    "        observation, \n",
    "        action, \n",
    "        reward, \n",
    "        new_observation, \n",
    "        done\n",
    "    ):\n",
    "        self.experience_replay.store(observation, action, reward, new_observation, done)\n",
    "\n",
    "\n",
    "    def sample_data(\n",
    "        self, \n",
    "        batch_size\n",
    "    ):\n",
    "        batch = self.experience_replay.sample_batch(batch_size)\n",
    "        observation_tensor = torch.Tensor(batch[\"observation_1\"]).to(self.device)\n",
    "        observation_next_tensor = torch.Tensor(batch[\"observation_2\"]).to(self.device)\n",
    "        actions_tensor = torch.Tensor(batch[\"actions\"]).to(self.device)\n",
    "        rewards_tensor = torch.Tensor(batch[\"rewards\"]).unsqueeze(1).to(self.device)\n",
    "        done_tensor = torch.Tensor(batch[\"done\"]).unsqueeze(1).to(self.device)\n",
    "        return observation_tensor, observation_next_tensor, actions_tensor, rewards_tensor, done_tensor\n",
    "\n",
    "\n",
    "    def get_probabilistic_Ms(\n",
    "        self, \n",
    "        Ms\n",
    "    ):\n",
    "        floored_Ms = np.floor(Ms)\n",
    "        if Ms - floored_Ms > 0.001:\n",
    "            prob_for_higher_value = Ms - floored_Ms\n",
    "            if np.random.uniform(0, 1) < prob_for_higher_value:\n",
    "                return int(floored_Ms + 1)\n",
    "            else:\n",
    "                return int(floored_Ms)\n",
    "        else:\n",
    "            return Ms\n",
    "\n",
    "\n",
    "    def get_redq_q_target_no_grad(\n",
    "        self, \n",
    "        observation_next_tensor, \n",
    "        rewards_tensor, \n",
    "        done_tensor\n",
    "    ):\n",
    " \n",
    "        Ms_to_use = self.get_probabilistic_Ms(self.M)\n",
    "        sample_idxs = np.random.choice(self.N, Ms_to_use, replace=False)\n",
    " \n",
    "        with torch.no_grad():\n",
    "            a_tilda_next, _, _, log_prob_a_tilda_next, _, _ = self.policy_network.forward(observation_next_tensor)\n",
    "            q_prediction_next_list = []\n",
    "            for sample_idx in sample_idxs:\n",
    "                q_prediction_next = self.q_target_networks_list[sample_idx](torch.cat([observation_next_tensor, a_tilda_next], 1))\n",
    "                q_prediction_next_list.append(q_prediction_next)\n",
    "            q_prediction_next_cat = torch.cat(q_prediction_next_list, 1)\n",
    "            min_q, min_indices = torch.min(q_prediction_next_cat, dim=1, keepdim=True)\n",
    "            next_q_with_log_prob = min_q - self.alpha * log_prob_a_tilda_next\n",
    "            y_q = rewards_tensor + self.gamma * (1 - done_tensor) * next_q_with_log_prob\n",
    " \n",
    "        return y_q, sample_idxs\n",
    "\n",
    "\n",
    "    def polyak_update_target_network(\n",
    "        self, \n",
    "        model_a, \n",
    "        model_b, \n",
    "        rou\n",
    "    ):\n",
    "        for model_a_param, model_b_param in zip(model_a.parameters(), model_b.parameters()):\n",
    "            model_a_param.data.copy_(rou * model_a_param.data + (1 - rou) * model_b_param.data)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        num_update = 0 if self.__get_current_num_data() <= self.delay_update_steps else self.G\n",
    "        for i_update in range(num_update):\n",
    "            observation_tensor, observation_next_tensor, actions_tensor, rewards_tensor, done_tensor = self.sample_data(self.batch_size)\n",
    "\n",
    "            # Q LOSS\n",
    "            y_q, sample_idxs = self.get_redq_q_target_no_grad(observation_next_tensor, rewards_tensor, done_tensor)\n",
    "            q_prediction_list = []\n",
    "            for q_i in range(self.N):\n",
    "                q_prediction = self.q_networks_list[q_i](torch.cat([observation_tensor, actions_tensor], 1))\n",
    "                q_prediction_list.append(q_prediction)\n",
    "            q_prediction_cat = torch.cat(q_prediction_list, dim=1)\n",
    "            y_q = y_q.expand((-1, self.N)) if y_q.shape[1] == 1 else y_q\n",
    "            q_loss_all = self.loss(q_prediction_cat, y_q) * self.N\n",
    "\n",
    "            for q_i in range(self.N):\n",
    "                self.q_optimizer_list[q_i].zero_grad()\n",
    "            q_loss_all.backward()\n",
    "\n",
    "            # POLICY LOSS\n",
    "            if ((i_update + 1) % self.policy_update_delay == 0) or i_update == num_update - 1:\n",
    "                # get policy loss\n",
    "                a_tilda, mean_a_tilda, log_std_a_tilda, log_prob_a_tilda, _, pretanh = self.policy_network.forward(observation_tensor)\n",
    "                q_a_tilda_list = []\n",
    "                for sample_idx in range(self.N):\n",
    "                    self.q_networks_list[sample_idx].requires_grad_(False)\n",
    "                    q_a_tilda = self.q_networks_list[sample_idx](torch.cat([observation_tensor, a_tilda], 1))\n",
    "                    q_a_tilda_list.append(q_a_tilda)\n",
    "                q_a_tilda_cat = torch.cat(q_a_tilda_list, 1)\n",
    "                ave_q = torch.mean(q_a_tilda_cat, dim=1, keepdim=True)\n",
    "                policy_loss = (self.alpha * log_prob_a_tilda - ave_q).mean()\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                for sample_idx in range(self.N):\n",
    "                    self.q_networks_list[sample_idx].requires_grad_(True)\n",
    "\n",
    "                # ALPHA LOSS\n",
    "                alpha_loss = -(self.log_alpha * (log_prob_a_tilda + self.target_entropy).detach()).mean()\n",
    "                self.alpha_optim.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optim.step()\n",
    "                self.alpha = self.log_alpha.cpu().exp().item()\n",
    "\n",
    "\n",
    "            for q_i in range(self.N):\n",
    "                self.q_optimizer_list[q_i].step()\n",
    "\n",
    "            if ((i_update + 1) % self.policy_update_delay == 0) or i_update == num_update - 1:\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "            for q_i in range(self.N):\n",
    "                self.polyak_update_target_network(self.q_target_networks_list[q_i], self.q_networks_list[q_i], self.polyak)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "\n",
    "observation_space = env.observation_space.shape[0] # Number of dimensions in the observation space\n",
    "action_space = env.action_space.shape[0] # Number of dimensions in the action space\n",
    "maximum_action = env.action_space.high[0].item() # Maximum value of the action\n",
    "hidden_dimensions = (256, 256) # Tuple of integers representing the number of neurons in each hidden layer of the neural networks\n",
    "hidden_activation = F.relu # Activation function used for the hidden layers of the neural networks\n",
    "action_bound_epsilon = 0.000001 # A small number used to prevent division by zero\n",
    "log_sig_min = -20 # The minimum value for the logarithm of the standard deviation of the action distribution\n",
    "log_sig_max = 2 # The maximum value for the logarithm of the standard deviation of the action distribution\n",
    "\n",
    "N = 5 # The number of Q-networks used for the REDQ algorithm\n",
    "M = 2 # The number of Q-values used for the REDQ algorithm at each iteration\n",
    "G = 10 # The ratio of updates to environment steps used for the REDQ algorithm\n",
    "\n",
    "replay_size = 1_000_000 # The size of the replay buffer used for experience replay\n",
    "batch_size = 256 # The number of samples taken from the replay buffer for each training iteration\n",
    "\n",
    "random_action_steps = 4_000 # The number of steps during which the agent takes random actions\n",
    "policy_update_delay = 40 # The frequency with which the policy network is updated\n",
    "\n",
    "alpha = 0.2 # The temperature parameter used for the policy's entropy term\n",
    "gamma = 0.99 # The discount factor for future rewards\n",
    "polyak = 0.995 # The polyak averaging parameter used to update the target networks\n",
    "\n",
    "learning_rate = 0.0003 # The learning rate used for the optimizer\n",
    "\n",
    "max_episodes = 1_000 # The maximum number of episodes to run the training loop for\n",
    "max_timesteps = 2_000 # The maximum number of time steps in each episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "\n",
    "ep_reward = 0\n",
    "reward_list = []\n",
    "plot_data = []\n",
    "log_f = open(\"agent-log.txt\",\"w+\")\n",
    "\n",
    "\n",
    "# Initialization\n",
    "agent = Agent(\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    maximum_action=maximum_action,\n",
    "    hidden_dimensions=hidden_dimensions,\n",
    "    hidden_activation=hidden_activation,\n",
    "    action_bound_epsilon=action_bound_epsilon,\n",
    "    log_sig_min=log_sig_min,\n",
    "    log_sig_max=log_sig_max,\n",
    "    N=N,\n",
    "    M=M,\n",
    "    G=G,\n",
    "    replay_size=replay_size,\n",
    "    batch_size=batch_size,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    polyak=polyak,\n",
    "    random_action_steps=random_action_steps,\n",
    "    policy_update_delay=policy_update_delay,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device    \n",
    ")\n",
    "\n",
    "\n",
    "# Training procedure\n",
    "for episode in range(1, max_episodes+1):\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    for t in range(max_timesteps):\n",
    "\n",
    "        # Select the agent action\n",
    "        action = agent.get_exploration_action(observation, env)\n",
    "\n",
    "        # Take action in the environment, obtain new_observation and reward\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Feed agent the new data\n",
    "        agent.store_data(observation, action, reward, next_observation, done)\n",
    "\n",
    "        # Let agent update\n",
    "        agent.train()\n",
    "\n",
    "        # Set observation to next observation\n",
    "        observation = next_observation\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        # Stop iterating when the episode finished\n",
    "        if done or t==(max_timesteps-1):\n",
    "            break\n",
    "    \n",
    "    # append the episode reward to the reward list\n",
    "    reward_list.append(ep_reward)\n",
    "\n",
    "    # do NOT change this logging code - it is used for automated marking!\n",
    "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
    "    log_f.flush()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    # print reward data every so often - add a graph like this in your report\n",
    "    if episode % plot_interval == 0:\n",
    "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
    "        reward_list = []\n",
    "        # plt.rcParams['figure.dpi'] = 100\n",
    "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.ylabel('Episode reward')\n",
    "        plt.show()\n",
    "        disp.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4f9117c26a772afbd48e6882cc440cc310fd82db52dd04a85ca50f3792ad396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
